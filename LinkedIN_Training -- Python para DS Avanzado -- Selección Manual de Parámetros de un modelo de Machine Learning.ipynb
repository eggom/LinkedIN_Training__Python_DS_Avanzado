{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección Manual de Parámetros\n",
    "\n",
    "### (en un modelo de Machine Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Libs\n",
    "\n",
    "# (En este caso usaremos Redes Neuronales pero este programa es adaptable a multitud de modelos estimadores)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos dataset\n",
    "\n",
    "data = pd.read_csv(\"../eggomPY_Datasets/AirlineDelays/DelayedFlights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino las cols\n",
    "\n",
    "col_pred = [\"AirTime\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"DepDelay\"]\n",
    "col_target = \"ArrDelay\"\n",
    "\n",
    "# Limpiamos el dataset de NaN\n",
    "\n",
    "df = data.dropna(subset=col_pred)\n",
    "df = df.dropna(subset=[\"ArrDelay\"])\n",
    "\n",
    "# Sampleamos el dataset y lo capamos a un total de 1000 filas.\n",
    "\n",
    "df = df.sample(frac=1,replace=True, random_state=12).head(1000)\n",
    "\n",
    "# Defino un conjunto X con los datos de predicción y un conjunto con los datos objetivo\n",
    "\n",
    "X = df[col_pred]\n",
    "y = df[col_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo conjuntos de train y testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un transformador escalador\n",
    "\n",
    "escalador = StandardScaler()\n",
    "\n",
    "###  IMPORTANTE!!!  ##############################################################\n",
    "###                                                                             ##\n",
    "###  Fiteo el escalador con el conjunto de variables predictoras de TRAINING!!! ##\n",
    "\n",
    "escalador.fit(X_train)\n",
    "\n",
    "# Aplico el transformador escalador al conjunto de testing y training\n",
    "\n",
    "X_train = escalador.transform(X_train)\n",
    "X_test = escalador.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo unas listas con valores arbitrarios para ciertos parámetros del modelo.\n",
    "\n",
    "# Recordar que en este caso el modelo es una red neuronal por tanto los hiperparámetros de mayor impacto serán:\n",
    "#\n",
    "# alphas: parametro de regularización/penalización (de la complejidad), para reducir posible overfitting.\n",
    "# layers: capas de la red neuronal.\n",
    "# solvers: método de solving (calculo numérico) dentro de la red neuronal para dar con la solución estable.\n",
    "\n",
    "alphas = [0.000001, 0.0001, 0.01, 0.1]\n",
    "layers = [2, 5, 10, 50, 100]\n",
    "solvers = [\"lbfgs\", \"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En total generaremos 40 modelos (fruto de la combinación de hiperparámetros):\n",
    "\n",
    "len(alphas)*len(layers)*len(solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.9728416523843435 Para Solver:  lbfgs  n_Layers:  2  Alpha:  1e-06\n",
      "R2 score:  0.9721800038097603 Para Solver:  adam  n_Layers:  2  Alpha:  1e-06\n",
      "R2 score:  0.9765075417734168 Para Solver:  lbfgs  n_Layers:  5  Alpha:  1e-06\n",
      "R2 score:  0.9723276810842592 Para Solver:  adam  n_Layers:  5  Alpha:  1e-06\n",
      "R2 score:  0.9763500230429344 Para Solver:  lbfgs  n_Layers:  10  Alpha:  1e-06\n",
      "R2 score:  0.9731723855774527 Para Solver:  adam  n_Layers:  10  Alpha:  1e-06\n",
      "R2 score:  0.9551501266522242 Para Solver:  lbfgs  n_Layers:  50  Alpha:  1e-06\n",
      "R2 score:  0.9763360388280711 Para Solver:  adam  n_Layers:  50  Alpha:  1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\30.Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.8987194608406996 Para Solver:  lbfgs  n_Layers:  100  Alpha:  1e-06\n",
      "R2 score:  0.9752994536593261 Para Solver:  adam  n_Layers:  100  Alpha:  1e-06\n",
      "R2 score:  0.9731423476799245 Para Solver:  lbfgs  n_Layers:  2  Alpha:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\30.Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.009256587581210063 Para Solver:  adam  n_Layers:  2  Alpha:  0.0001\n",
      "R2 score:  0.9739554788998845 Para Solver:  lbfgs  n_Layers:  5  Alpha:  0.0001\n",
      "R2 score:  0.9731618884889088 Para Solver:  adam  n_Layers:  5  Alpha:  0.0001\n",
      "R2 score:  0.9754778142904855 Para Solver:  lbfgs  n_Layers:  10  Alpha:  0.0001\n",
      "R2 score:  0.9746809698631999 Para Solver:  adam  n_Layers:  10  Alpha:  0.0001\n",
      "R2 score:  0.962780915246534 Para Solver:  lbfgs  n_Layers:  50  Alpha:  0.0001\n",
      "R2 score:  0.9752658448920241 Para Solver:  adam  n_Layers:  50  Alpha:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\30.Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.8631933899160615 Para Solver:  lbfgs  n_Layers:  100  Alpha:  0.0001\n",
      "R2 score:  0.974000198961827 Para Solver:  adam  n_Layers:  100  Alpha:  0.0001\n",
      "R2 score:  0.9727183019903296 Para Solver:  lbfgs  n_Layers:  2  Alpha:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\30.Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.025284525489700282 Para Solver:  adam  n_Layers:  2  Alpha:  0.01\n",
      "R2 score:  0.9787325262294504 Para Solver:  lbfgs  n_Layers:  5  Alpha:  0.01\n",
      "R2 score:  0.9754218952195527 Para Solver:  adam  n_Layers:  5  Alpha:  0.01\n",
      "R2 score:  0.9772680307579051 Para Solver:  lbfgs  n_Layers:  10  Alpha:  0.01\n",
      "R2 score:  0.975434214954638 Para Solver:  adam  n_Layers:  10  Alpha:  0.01\n",
      "R2 score:  0.9555692857115502 Para Solver:  lbfgs  n_Layers:  50  Alpha:  0.01\n",
      "R2 score:  0.9749379021074807 Para Solver:  adam  n_Layers:  50  Alpha:  0.01\n",
      "R2 score:  0.9022579037938081 Para Solver:  lbfgs  n_Layers:  100  Alpha:  0.01\n",
      "R2 score:  0.9742697773308048 Para Solver:  adam  n_Layers:  100  Alpha:  0.01\n",
      "R2 score:  0.9737727024932346 Para Solver:  lbfgs  n_Layers:  2  Alpha:  0.1\n",
      "R2 score:  0.9723177335998678 Para Solver:  adam  n_Layers:  2  Alpha:  0.1\n",
      "R2 score:  0.9794523835160901 Para Solver:  lbfgs  n_Layers:  5  Alpha:  0.1\n",
      "R2 score:  0.9739505905153579 Para Solver:  adam  n_Layers:  5  Alpha:  0.1\n",
      "R2 score:  0.9777452780912261 Para Solver:  lbfgs  n_Layers:  10  Alpha:  0.1\n",
      "R2 score:  0.9756237800241285 Para Solver:  adam  n_Layers:  10  Alpha:  0.1\n",
      "R2 score:  0.9679498049465095 Para Solver:  lbfgs  n_Layers:  50  Alpha:  0.1\n",
      "R2 score:  0.9741960195898464 Para Solver:  adam  n_Layers:  50  Alpha:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\30.Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:  0.9230416044910194 Para Solver:  lbfgs  n_Layers:  100  Alpha:  0.1\n",
      "R2 score:  0.9754043270757711 Para Solver:  adam  n_Layers:  100  Alpha:  0.1\n"
     ]
    }
   ],
   "source": [
    "# Creamos el loop: (Soluc. rudimentaria pero efectiva)\n",
    "\n",
    "for alpha in alphas:\n",
    "    for layer in layers:\n",
    "        for solver in solvers:\n",
    "                clf = MLPRegressor(solver=solver, alpha=alpha, hidden_layer_sizes=(layer,), warm_start=True, max_iter=10000)\n",
    "                model = clf.fit(X_train, y_train)\n",
    "                y_guess = model.predict(X_test)\n",
    "                print(\"R2 score: \",r2_score(y_test, y_guess),\"Para Solver: \",solver,\" n_Layers: \",layer,\" Alpha: \",alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente SciKit Learn tiene una manera más elegante de resolver esto sin tener que recurrir a estos loops..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
